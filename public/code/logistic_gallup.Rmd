---
title: "Logistic Regression in R: Catholic Opinions on Celibate Clergy"
author: John A. Bernau
date: "6/26/2018"
slug: logistic_gallup
categories: []
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<a name="top"></a>
This post offers an example of performing logistic regression in R. I'll cover the following topics:

<div style= "float:right;position: relative; top: -10px; left: 10px;">
<img src="http://prodimage.images-bn.com/pimages/9780316055697_p0_v5_s1200x630.jpg" height="400" />
</div>
- [Importing data](#import)
- [Preparing data](#prep)
- [Fitting logistic model](#model)
- [Interpreting Coefficients](#coef)
- [Assessing model fit](#fit)
- Generating predicted probabilities
- Plotting!

More specifically, I'll be using the [2005 Gallup Poll of Catholics](http://www.thearda.com/Archive/Files/Descriptions/GALLUP05.asp) hosted on the [ARDA data archives](http://www.thearda.com/Archive/Files/Downloads/GALLUP05_DL2.asp), to examine the factors that drive catholic opinion on the importance of celibate clergy. This issue became one of national importance when the Boston Globe unearthed the systematic abuse of minors by catholic clery in 2002 (as described in [their Pulitzer-Prize-winning book](https://www.amazon.com/Betrayal-Catholic-findings-investigation-Spotlight/dp/0316271535)). Gallup conducted multiple waves of its catholic poll (1987, 1992, 1993, 1999, and 2005), but I'll only focus on the most recent wave and set aside any questions of longitudinal trends for now. 

***RQ: How is one's support for priestly celibacy influenced by factors such as age, sex, politics, mass attendance, or education?***

___


```{r, message = FALSE}
# Loading required pacakges:
require("tidyverse")
require("foreign")
require("broom")
require("RColorBrewer")
require("scales")
```

___ 

<a name="import"></a>
**Importing Data**  

___


We'll use `foreign::read.dta()` to read the labeled Stata file directly into R from it's URL. 

```{r}
gallup <- read.dta("http://www.thearda.com/download/download.aspx?file=Gallup%20Poll%20of%20Catholics,%202005.DTA") %>% 
  as_tibble()
```

An initial glimpse gives us an idea of the 96 different variables, but we'll need [the codebook](http://www.thearda.com/Archive/Files/Downloads/GALLUP05_DL2.asp) to decipher which variables / responses are of interest. 
```{r, eval = FALSE}
glimpse(gallup)
```

___

<a name="prep"></a>
**Preparing Data**   

___
<div style="text-align: right">[Back to Top](#top)</div> 
<br>
Let's start by examining age. It seems reasonable to expect support for a somewhat traditional belief to vary across generations. First, I remove any missing values and then plot a histogram. We can see a fairly normal distirbution centered around the mean of 52.

```{r, message=FALSE}
# Remove missing values of age
gallup <- gallup %>% 
  filter(age!= "NA")

# Plot histogram of age, with vertical line at mean
ggplot(gallup, aes(age)) + 
  geom_histogram() +
  geom_vline(xintercept = mean(gallup$age), color = "red")
```


By the same logic, we can include `education` and `sex` to our list of relevant variables.

```{r}
# Bar chart: education X sex
ggplot(gallup, aes(educ)) + 
  geom_bar(aes(fill=sex), stat="count", position="dodge") + 
  coord_flip()
```

It looks like there are slightly more women than men in our sample, and a fairly even distribution across levels of education. Let's get rid of the cumbersome labels with a bit of recoding. First, examine the default levels of the education variable.

```{r}
levels(gallup$educ)
```

There are many ways to recode, but I find base R's functions intuitive enough. After defining a new variable `educ2`, the following code is best read from inside out. For example, if the original `educ` variable equals "Less than a high school graduate (0-11)" we give our new variable `educ2` a 1. Likewise, high school graduates get a value of 2. Producing a table of our new variable shows a clean output of each of our five new levels. 
```{r}
# Using base R is a bit more intuitive for me.
# Read from inside out. If educ == X then assign NA to gallup$educ2
gallup$educ2 <- NA
gallup$educ2[gallup$educ == 'Less than a high school graduate (0-11)'] <- 1
gallup$educ2[gallup$educ == 'High school graduate (12)'] <- 2
gallup$educ2[gallup$educ == 'Trade/technical/vocational training'] <- 3
gallup$educ2[gallup$educ == 'Some college'] <- 3
gallup$educ2[gallup$educ == 'College graduate'] <- 4
gallup$educ2[gallup$educ == 'Postgraduate work or academic or professional degree'] <- 5

table(gallup$educ2)

```

To finish, we can assign these levels new labels and save as a new factor variable. The final table shows our clean and recoded education variable. 
```{r}
# Generate new factor labels
gallup$educ2 <- factor(gallup$educ2, labels = c("Less than HS", "High School", "Trade / Some col", "College", "Professional"))

# Remove any missing values
gallup <- filter(gallup, !is.na(educ2))

# Space permitting, always examine your original vs your recoded variable:
# table(gallup$educ2, gallup$educ)

# Examine final table
table(gallup$educ2)


```

Replotting our bar chart shows the collapsed categories and new succinct labels.
```{r}
ggplot(gallup, aes(educ2)) + 
  geom_bar(aes(fill=sex), stat="count", position="dodge") + 
  coord_flip()
```

To avoid confusion later on, I want to make sex a dichotomous variable. 
```{r}
# Initialize new variable
gallup$female <- NA
# Recode
gallup$female[gallup$sex == "Male"] <- 0
gallup$female[gallup$sex == "Female"] <- 1
# Double check
table(gallup$female, gallup$sex)
```

Another variable of interest is the respondent's political party identification. Are republicans more supportive of priestly chastisty? Do political independents have strong opinions? An initial table shows five levels, of which we'll only keep the first three. 

```{r}
# Initial examination
table(gallup$partyid)

# Keep only those respondents in the three desired categories
gallup <- filter(gallup, partyid == "Republican" | partyid == "Democrat" | partyid == "Independent")

# Visual summary
ggplot(gallup, aes(partyid)) + 
  geom_bar(stat="count", position="dodge", aes(fill = partyid))
```

Most sociological studies of religion rely on some sort of attendance measure, and it makes sense to include it here. How often you attend mass surely affects your opinions of what goes on inside! This variable is intuitively coded, so all we'll do to prepare is remove the "Don't Knows".

```{r}
table(gallup$attend)
gallup <- filter(gallup, attend != "Don't know")
```

The last variable I want to include is US geographic region. There are huge demographic differences in the concentration of US Catholics. While the northeast was home to most catholics in American history, the influx of immigrants from Latin America has shifted the power balance to the southern states. Furthermore, one's opinion of celibate clergy may be related to the local concentration of catholics, or the distance from the nearest headline-grabbing diocese. Luckily, this variable is already clean and ready to go. 
```{r}
table(gallup$region)
ggplot(gallup, aes(region)) +
  geom_bar(stat = "count", aes(fill = region))
```

Moving to our dependent variable, we have a four-category response to the question:  
> As a Catholic, how important is each of the following to you?  Would you say the following is or are very important, somewhat important, or not important at all?  E. A celibate male clergy

```{r}
table(gallup$celclerg)
```

Because we're using a logistic regression, our response must be in binary form. Ideally, we would use a multinomial model to preserve the variation given by a three-level variable, but for our purposes, we'll collapse into two categories along an "important" vs "not important" dichotomy. We could also reasonably drop the "somewhat important" category to leave just those that feel strongly either way. No matter the decision, make sure to justify your rationale! 
```{r}
# Initialize new variable "celimp"
gallup$celimp <- NA

# Recode 
gallup$celimp[gallup$celclerg == 'Not important at all'] <- 0
gallup$celimp[gallup$celclerg == 'Somewhat important'] <- 1
gallup$celimp[gallup$celclerg == 'Very important'] <- 1
table(gallup$celimp)

# Filter only those in our new categories
gallup <- filter(gallup, celimp == 0 | celimp == 1)
```



Our prepared dataset should have 819 observations of 98 variables after cleaning measures of age, education, sex, attendance, political identification and US region. 

```{r}
# A marginally helpful heatmap of four categorical variables
ggplot(gallup, aes(partyid, attend)) +
  geom_tile(alpha=0.1, fill = "darkblue") +
  facet_grid(sex~region) +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.5))

```

___

<a name="model"></a>
**Fitting Logistic Model**  

___
<div style="text-align: right">[Back to Top](#top)</div> 
<br>
With a clean dataset, we can run our logistic model with the following code. `glm` for generalized linear model with the arguments:  

> glm(DV ~ IV1 + IV2 + IV3..., data = dataset, family = binomial("logit"))

Make sure to specify which variables to treat as categorical using the `factor()` notation.

```{r}
model <- glm(celimp ~ female + factor(educ2) + age + factor(partyid) + factor(region) + factor(attend), data = gallup, family = binomial("logit"))
```

Our logistic model is now saved as an object in R. Using base R, we can print a summary to the console with the following code.
```{r}
summary(model)
```

___

<a name="coef"></a>
**Interpreting Coefficients**   

___
<div style="text-align: right">[Back to Top](#top)</div> 
<br>
Before turning to substantive interpretation, let's get our model in a readable format. As [David Robinson notes](http://varianceexplained.org/r/broom-intro/), the printed summary isn't exactly a format we're used to working with. Luckily the `broom` package converts statistical models into tidy data frames. Feeding our model through `tidy()` gives us a data frame for our coefficients, with each row corresponding to a regression term and each column providing a generated statistic: slope estimate, standard error, p-values, confidence intervals, etc. 

```{r}
mod_coef <- tidy(model, conf.int = TRUE)
```

It may not look like much of a difference, but once in this format, we can apply all our tidy functions to the results of our model. For example, let's examine a plot of our coefficient estimates. 
```{r}
ggplot(mod_coef, aes(estimate, term, color = term)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  geom_vline(xintercept = 0) +
  scale_color_discrete(guide = FALSE)

```

If you're like me, estimates of the log-odds don't mean much. By exponentiating our coefficients (and confidence intervals), we can create odds ratios for easier interpretation. We can also add a new term `sig` for whether our confidence intervals overlap with 1 (our null value in terms of odds ratios). 

```{r, warning = FALSE}
mod_coef <- tidy(model, conf.int = TRUE) %>% 
  mutate(OR = exp(estimate),
         OR_ll = exp(conf.low),
         OR_ul = exp(conf.high),
         sig = ifelse(OR_ll < 1 & OR_ul <1 | OR_ll >1 & OR_ul >1, 
                      1, 
                      0))

# New plot with OR on x-axis
ggplot(mod_coef, aes(OR, term, color = sig)) +
  geom_point() + 
  geom_errorbarh(aes(xmin = OR_ll, xmax = OR_ul)) +
  geom_vline(xintercept = 1) +
  scale_x_continuous(limits = c(-0.5,2.5)) +
  scale_color_continuous(guide = FALSE)
```

Alternatively, you could color by p-value, with darker plots corresponding to "more significant" estiamtes. This method underscores the importance of treating significance tests as a contiuum rather than a binned range of "significant" and "non-significant" values. See [Sterne & Smith 2001](http://www.jayskaufman.com/uploads/3/0/8/9/30891283/sterne_&_davey_smith_whats_wrong_with_significance_tests_bmj.pdf).


```{r, warning = FALSE}
ggplot(mod_coef, aes(OR, term, color = p.value)) +
  geom_point() +
  geom_errorbarh(aes(xmin = OR_ll, xmax = OR_ul)) +
  geom_vline(xintercept = 1) +
  scale_x_continuous(limits = c(-0.5,2.5)) 
```

Lastly, we could express our estimates in terms of percentage change: "For every one-unit increase in X, respondents are X% more likely to say celibate clergy are important." Or for our categorical variables: "Compared to someone in category A, someone in category B is X% more likely to say celibate clergy are important." Subtracting one from our odds ratios and multiplying by 100 gives us this measure.  
```{r, warning = FALSE}
mod_coef <- mod_coef %>% 
  mutate(per_change = (OR-1)*100,
         per_ll = (OR_ll-1)*100,
         per_ul = (OR_ul-1)*100)

# Plotting. See text label for identifying relevant estimates
ggplot(mod_coef, aes(per_change, term, color = sig)) +
  geom_point() + 
  geom_errorbarh(aes(xmin = per_ll, xmax = per_ul)) +
  geom_text(aes(label = ifelse(sig == 1, round(per_change, 2), "")), nudge_x = -60) +
  geom_vline(xintercept = 1) +
  scale_x_continuous(limits = c(-150,150)) +
  scale_color_continuous(guide = FALSE)
```

Thus, we can easily see that compared to a republican (the reference category), a catholic democrat is 35.37% less likely to say celibate clergy are "very important."


STOPPED HERE....

```{r}
#####################
# AUGMENT function
#####################
# Create dataframe with fitted values (in proportions) plus residuals and stuff
aug_mod <- augment(model, type.predict = "response")

aug_mod <- augment(model)

head(aug_mod)
ggplot(aug_mod, aes(.resid, .fitted)) +
  geom_point()

################################
# Generate binary predictions
aug_mod <- aug_mod %>% 
  mutate(celimp_hat = round(.fitted))

# Confusion matrix
cm <- aug_mod %>% 
  select(celimp, celimp_hat) %>% 
  table() %>% 
  addmargins()
cm

# Sensitivity is the true positive rate. Predicted positives / Actual positives.
# 172/220 = 0.78
tpr <- cm[3,2] / cm[2,3]
x <- paste("True positive rate =", round(tpr, 3))

# Specificity is the true negative rate. Predicted negatives / actual negatives. 
# 396 / 348 = 1.14
tnr <- cm[3,1] / cm[1,3]
y <- paste("True negative rate =", round(tnr, 3))

# Overall
overall <- (cm[1,1] + cm[2,2]) / cm[3,3]
z <- paste("Overall accuracy =", round(overall, 3))

# Generate new variable: right or wrong
aug_mod <- aug_mod %>% 
  mutate(correct = ifelse(celimp == celimp_hat, 1, 0))

# Plotting stuff
ggplot(aug_mod, aes(factor.attend., .fitted)) +
  geom_jitter(width = 0.05, 
              alpha = 0.5, 
              aes(color = correct))
```

  