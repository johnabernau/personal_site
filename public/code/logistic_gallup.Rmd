---
title: "Logistic Regression in R: Catholic Opinions on Celibate Clergy"
author: John A. Bernau
date: '2018-06-27'
slug: logistic_gallup
categories: []
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<a name="top"></a>
This post offers an example of performing logistic regression in R. I'll cover the following topics:

<div style= "float:right;position: relative; top: -10px; left: 10px;">
<img src="http://prodimage.images-bn.com/pimages/9780316055697_p0_v5_s1200x630.jpg" height="400" />
</div>
- [Importing data](#import)
- [Preparing data](#prep)
- [Fitting logistic model](#model)
- [Interpreting Coefficients](#coef)
- [Assessing model fit](#fit)
- Generating predicted probabilities
- Plotting!

More specifically, I'll be using the [2005 Gallup Poll of Catholics](http://www.thearda.com/Archive/Files/Descriptions/GALLUP05.asp) hosted on the [ARDA data archives](http://www.thearda.com/Archive/Files/Downloads/GALLUP05_DL2.asp), to examine the factors that drive catholic opinion on the importance of celibate clergy. This issue became one of national importance when the Boston Globe unearthed the systematic abuse of minors by catholic clery in 2002 (as described in [their Pulitzer-Prize-winning book](https://www.amazon.com/Betrayal-Catholic-findings-investigation-Spotlight/dp/0316271535)). Gallup conducted multiple waves of its catholic poll (1987, 1992, 1993, 1999, and 2005), but I'll only focus on the most recent wave and set aside any questions of longitudinal trends for now. 

***RQ: How is one's support for priestly celibacy influenced by factors such as age, sex, politics, mass attendance, or education?***

___


```{r, message = FALSE}
# Loading required pacakges:
require("tidyverse")
require("foreign")
require("broom")
require("RColorBrewer")
require("scales")
```

___ 

<a name="import"></a>
**Importing Data**  

___


We'll use `foreign::read.dta()` to read the labeled Stata file directly into R from it's URL. 

```{r}
gallup <- read.dta("http://www.thearda.com/download/download.aspx?file=Gallup%20Poll%20of%20Catholics,%202005.DTA") %>% 
  as_tibble()
```

An initial glimpse gives us an idea of the 96 different variables, but we'll need [the codebook](http://www.thearda.com/Archive/Files/Downloads/GALLUP05_DL2.asp) to decipher which variables / responses are of interest. 

___

<a name="prep"></a>
**Preparing Data**   

___
<div style="text-align: right">[Back to Top](#top)</div> 
<br>
I'll be looking at six independent (or explanatory) variables: age, education, sex, political identification, mass attendance, and US geographic region. First, I remove any missing values or irrelevant categories. Second, I generate new variables and recode / collapse categories as appropriate. Lastly, I plot some descriptive graphs to examine our clean variables. 

```{r, message=FALSE}
# Remove missing values
gallup <- gallup %>% 
  filter(!is.na(age),
         !is.na(educ),
         partyid == "Republican" | partyid == "Democrat" | partyid == "Independent",
         attend != "Don't know")

# Recoding education
gallup$educ2 <- NA
gallup$educ2[gallup$educ == 'Less than a high school graduate (0-11)'] <- 1
gallup$educ2[gallup$educ == 'High school graduate (12)'] <- 2
gallup$educ2[gallup$educ == 'Trade/technical/vocational training'] <- 3
gallup$educ2[gallup$educ == 'Some college'] <- 3
gallup$educ2[gallup$educ == 'College graduate'] <- 4
gallup$educ2[gallup$educ == 'Postgraduate work or academic or professional degree'] <- 5

# Generate new factor labels for education
gallup$educ2 <- factor(gallup$educ2, labels = c("Less than HS", "High School", "Trade / Some col", "College", "Professional"))

# Remove any remaining NAs for education
gallup <- gallup %>% 
  filter(!is.na(educ2))

# Dichotomizing sex variable into "female"
gallup$female <- NA
gallup$female[gallup$sex == "Male"] <- 0
gallup$female[gallup$sex == "Female"] <- 1

```

Plotting a histogram of age, with vertical line at mean. 
```{r, warning = FALSE}
ggplot(gallup, aes(age)) + 
  geom_histogram() +
  geom_vline(xintercept = mean(gallup$age), color = "red")
```

Plotting a bar chart of education, filled according to sex. 
```{r}
ggplot(gallup, aes(educ2)) + 
  geom_bar(aes(fill=factor(female)), stat="count", position="dodge") + 
  coord_flip()
```

Our prepared dataset should have 819 observations of 98 variables after cleaning measures of age, education, sex, attendance, political identification and US region. 

```{r}
# A marginally helpful heatmap of four categorical variables
ggplot(gallup, aes(partyid, attend)) +
  geom_tile(alpha=0.1, fill = "darkblue") +
  facet_grid(sex~region) +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.5))

```





Moving to our dependent variable, we have a four-category response to the question:  

> As a Catholic, how important is each of the following to you?  Would you say the following is or are very important, somewhat important, or not important at all?  E. A celibate male clergy

```{r}
table(gallup$celclerg)
```

Because we're using a logistic regression, our response must be in binary form. Ideally, we would use a multinomial model to preserve the variation given by a three-level variable, but for our purposes, we'll collapse into two categories along an "important" / "not important" dichotomy. We could also reasonably drop the "somewhat important" category to leave just those that feel strongly either way. No matter the decision, make sure to justify your rationale! 
```{r}
# Initialize new variable "celimp"
gallup$celimp <- NA

# Recode 
gallup$celimp[gallup$celclerg == 'Not important at all'] <- 0
gallup$celimp[gallup$celclerg == 'Somewhat important'] <- 1
gallup$celimp[gallup$celclerg == 'Very important'] <- 1

# Filter only those in our new categories
gallup <- filter(gallup, celimp == 0 | celimp == 1)
```


Our prepared dataset should have 813 observations of 99 variables after cleaning measures of our dependent variable `celimp` : "importance of celibate clergy."

___

<a name="model"></a>
**Fitting Logistic Model**  

___
<div style="text-align: right">[Back to Top](#top)</div> 
<br>
With a clean dataset, we can run our logistic model with the following code. Make sure to specify which variables to treat as categorical using the `factor()` notation.

```{r}
model <- glm(celimp ~ female + factor(educ2) + age + factor(partyid) + factor(region) + factor(attend), data = gallup, family = binomial("logit"))
```


Our logistic model is now saved as an object in R. Using base R, we can print a summary to the console with the following code. [Output omitted]
```{r, eval = FALSE}
summary(model)
```

One thing we may want to change is the reference category of a categorical variable. By default, R chooses the first category, but you can set this to a specific value using the following code. In our case, each education category is tested against "Less than HS." Because this accounts for only a small number of our respondents, I change the reference category to catholics with a college education. 

```{r}
gallup$educ2 <- relevel(gallup$educ2, ref = "College")

model <- glm(celimp ~ female + factor(educ2) + age + factor(partyid) + factor(region) + factor(attend), data = gallup, family = binomial("logit"))

# summary(model)
```

Before turning to coefficients, we can get an overall sense of model-fit using `glance()` from the `broom` package. This reports overall log-Likelihood as well as AIC and BIC fit statistics. 
```{r}
glance(model)
```


___

<a name="coef"></a>
**Interpreting Coefficients**   

___
<div style="text-align: right">[Back to Top](#top)</div> 
<br>
Before turning to substantive interpretation, let's get our model in a readable format. As [David Robinson notes](http://varianceexplained.org/r/broom-intro/), the printed summary isn't exactly a format we're used to working with. Luckily the `broom` package converts statistical models into tidy data frames. Feeding our model through `tidy()` gives us a data frame for our coefficients, with each row corresponding to a regression term and each column providing a generated statistic: slope estimate, standard error, p-values, confidence intervals, etc. 

```{r}
mod_coef <- tidy(model, conf.int = TRUE)
```

It may not look like much of a difference, but once in this format, we can apply all our tidy functions to the results of our model. For example, let's examine a plot of our coefficient estimates. 
```{r}
ggplot(mod_coef, aes(estimate, term, color = term)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  geom_vline(xintercept = 0) +
  scale_color_discrete(guide = FALSE)

```

By including both the estimate and their confidence intervals using `geom_errorbarh()` this simple plot shows us most of the important output of our statistical model! By tweaking the ggplot code, we can start to see the benefit of this format. 

- Present estimates as odds ratios: OR = exp(estimate)
- Present estimates as percent change: per_change = (OR - 1) * 100
- Color by size of estimate, p-value, or any other grouping

Coloring by p-value, with darker plots corresponding to "more significant" estiamtes, is especially appealing by underscoring the importance of treating significance tests as a contiuum rather than a binned range of "significant" and "non-significant" values. See [Sterne & Smith 2001](http://www.jayskaufman.com/uploads/3/0/8/9/30891283/sterne_&_davey_smith_whats_wrong_with_significance_tests_bmj.pdf).

Below, I express estimates in terms of percentage change: "For every one-unit increase in X, respondents are Y% more likely to say celibate clergy are important." Or for our categorical variables: "Compared to someone in category A, someone in category B is Y% more likely to say celibate clergy are important." Subtracting one from our odds ratios and multiplying by 100 gives us this measure. 

Most of the preparation below is reordering the regression terms in logical way. First, I add four reference categories as `refs` and row-bind these to the existing data.frame `mod_coef`. Then I prepare our `terms` variable for cleaner plotting: remove messy prefixes, group by variable, and reorder the levels. The plot colors each set of coefficients by its parent variable and displays a text label of the estimate if the confidence intervals clear the null value. 

```{r, warning = FALSE, message=FALSE}
# Tidy estimates + new variables
mod_coef <- tidy(model, conf.int = TRUE) %>% 
  mutate(OR = exp(estimate),
         OR_ll = exp(conf.low),
         OR_ul = exp(conf.high),
         sig = ifelse(OR_ll < 1 & OR_ul <1 | OR_ll >1 & OR_ul >1, 
                      1, 
                      0),
         per_change = (OR-1)*100,
         per_ll = (OR_ll-1)*100,
         per_ul = (OR_ul-1)*100)

# Add reference categories for plotting
refs <- data.frame(term = c("East", "Republican", "College", "Weekly"), 
                   per_change = c(0,0,0,0))
# Row-bind
require(plyr)
require(dplyr)
plot <- rbind.fill(mod_coef, refs)

# Prepare terms for plotting
# Remove "factor"" prefix
plot$term <- str_replace_all(plot$term, "factor", "")

# Generate "vars" to distinguish groups of categorical terms
plot$vars <- "rest"
plot$vars[plot$term == "Weekly"] <- "attend"
plot$vars[str_detect(plot$term, "attend")] <- "attend"
plot$vars[plot$term == "East"] <- "region"
plot$vars[str_detect(plot$term, "region")] <- "region"
plot$vars[plot$term == "Republican"] <- "party"
plot$vars[str_detect(plot$term, "partyid")] <- "party"
plot$vars[plot$term == "College"] <- "educ"
plot$vars[str_detect(plot$term, "educ2")] <- "educ"

# Remove prefixes from term labels
plot$term <- str_replace_all(plot$term, "\\(attend\\)", "")
plot$term <- str_replace_all(plot$term, "\\(region\\)", "")
plot$term <- str_replace_all(plot$term, "\\(partyid\\)", "")
plot$term <- str_replace_all(plot$term, "\\(educ2\\)", "")
plot$term <- str_replace_all(plot$term, "\\(Intercept\\)", "Intercept")

# Reorder the levels of our factor
plot$term <- factor(plot$term, levels = c("East", "Midwest", "West", "South", "Weekly", "Two or three times a month", "About once a month", "A few times a year", "Seldom or never", "Republican", "Democrat", "Independent", "Less than HS", "High School", "Trade / Some col", "College", "Professional", "female", "age", "Intercept"))
```

```{r, eval = FALSE}
# Plotting
ggplot(plot, aes(per_change, fct_rev(term), color = vars)) +
  geom_point(size = 2) +
  geom_errorbarh(aes(xmin = per_ll, xmax = per_ul)) +
  geom_text(aes(label = ifelse(sig == 1, round(per_change, 2), "")), 
            nudge_x = -50) +
  geom_vline(xintercept = 0) +
  scale_x_continuous(limits = c(-150,150), 
                     breaks = seq(from = -150, to = 150, by = 50)) +
  scale_color_brewer(palette = "Set1", guide = FALSE) +
  labs(x = "Percent Change in Pr(DV)", y = "", 
       title = "Celibate Clergy as \"Very Important\"", 
       subtitle = "Logistic Regression estimates", 
       caption = "\nData source: Gallup Poll of Catholics 2005") +
   theme(text=element_text(size = 14, family="Times New Roman"),
        axis.text = element_text(size = 10),
        plot.caption = element_text(size = 9),
        panel.background = element_rect(fill="white"),
        panel.grid.minor = element_line(color="grey90"),
        panel.grid.major = element_line(color="grey90"),
        plot.margin = unit(c(0.5,0.5,0.5,0.5), "cm"))
```

[<img src = "/code/lr_gall.jpg" height = "650" align="left">](/code/lr_gall.jpg)

Thus, we can easily see the attendance measures in red, the political measures in green, and the education in blue, with each reference category sitting on the 0% change line. This allows an intuitive way to see the effect of moving between each category. Compared to a republican, a catholic democrat is 35.37% less likely to say celibate clergy are "very important." Compared to a college-educated catholic, those with a professional degree are 46.93% less likely to say celibate clergy are "very important." Remember, these estimates emerge after controlling for every other variable in the model. The effect of education is independent of political identification or attendance. 

STOPPED HERE....

```{r}
#####################
# AUGMENT function
#####################
# Create dataframe with fitted values (in proportions) plus residuals and stuff
aug_mod <- augment(model, type.predict = "response")

aug_mod <- augment(model)

head(aug_mod)
ggplot(aug_mod, aes(.resid, .fitted)) +
  geom_point()

################################
# Generate binary predictions
aug_mod <- aug_mod %>% 
  mutate(celimp_hat = round(.fitted))

# Confusion matrix
cm <- aug_mod %>% 
  select(celimp, celimp_hat) %>% 
  table() %>% 
  addmargins()
cm

# Sensitivity is the true positive rate. Predicted positives / Actual positives.
# 172/220 = 0.78
tpr <- cm[3,2] / cm[2,3]
x <- paste("True positive rate =", round(tpr, 3))

# Specificity is the true negative rate. Predicted negatives / actual negatives. 
# 396 / 348 = 1.14
tnr <- cm[3,1] / cm[1,3]
y <- paste("True negative rate =", round(tnr, 3))

# Overall
overall <- (cm[1,1] + cm[2,2]) / cm[3,3]
z <- paste("Overall accuracy =", round(overall, 3))

# Generate new variable: right or wrong
aug_mod <- aug_mod %>% 
  mutate(correct = ifelse(celimp == celimp_hat, 1, 0))

# Plotting stuff
ggplot(aug_mod, aes(factor.attend., .fitted)) +
  geom_jitter(width = 0.05, 
              alpha = 0.5, 
              aes(color = correct))
```

  